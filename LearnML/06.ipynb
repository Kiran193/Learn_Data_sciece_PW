{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWsdiIFTd6rc"
   },
   "source": [
    "\n",
    "# Machine Learning Practice - Asynchronous\n",
    "## Homework 06: Cross Validation\n",
    "\n",
    "## Assignment Overview\n",
    "First read through the entire notebook & do not write any code. This assignment\n",
    "is more complex than previous ones, and it will be helpful to have a sense of \n",
    "the structure before you start coding.  \n",
    "\n",
    "Follow the TODOs and read through and understand any provided code.  \n",
    "All the plotting functions have been provided. You should not need to alter\n",
    "any of these.\n",
    "\n",
    "### Task\n",
    "For this assignment you will be implementing __holistic cross-validation__. \n",
    "Cross-validation is a procedure that involves training, validating, and testing \n",
    "a model on different subsets of the data set to evaluate how well the model will \n",
    "generalize to unseen examples. Additionally, cross validation is a good tool \n",
    "for evaulating models when only small amounts of data are available.  \n",
    "\n",
    "The training set is used to train the parameters of the various models; the validation \n",
    "set is used to evaluate and select the best performing model hyper-parameter set, and the\n",
    "testing set is used to perform evaluation of the selected models (but only after hyper-parameters\n",
    "are selected!).\n",
    "\n",
    "On top of cross-validation, we will be experimenting with differently sized training sets and \n",
    "different sets of hyper-parameters for your model.  This means that we will be executing a \n",
    "3D grid of experiments that is indexed by:\n",
    "1. Rotation\n",
    "2. Hyper-parameter set\n",
    "3. Training set size\n",
    "\n",
    "Each of the experiments in the grid involves the training and evaluation of a model, as well as collating\n",
    "these results into a form through which it is easy to identify the ideal hyper-parameters for a given\n",
    "training set size.  Specifically, for each training set size, we will select the one hyper-parameter set that \n",
    "maximizes the mean validation performance (across the rotations).  Once a hyper-parameter set is chosen, we \n",
    "will then examine the mean test set performance (again, for each training set size).\n",
    "\n",
    "### Data set\n",
    "The BMI (Brain Machine Interface) data are stored in a single pickle file; within this file, there is one dictionary that contains all of the data.  The keys are: 'MI', \n",
    "'theta', 'dtheta', 'ddtheta', 'torque', and 'time'.  Each of these objects are python lists with 20 \n",
    "numpy matrices; each matrix contains an independent fold of data, with rows representing \n",
    "different samples and columns representing different features.  The samples are organized \n",
    "contiguously (one sample every 50ms), but there are gaps in the data.\n",
    "* _MI_ contains the data for 48 neurons.  Each row encodes the number of action potentials for \n",
    "each neuron at each of 20 different time bins (so, 48 x 20 = 960 columns).  \n",
    "* _theta_ contains the angular position of the shoulder (in column 0) and the elbow \n",
    "(in column 1) for each sample.  \n",
    "* _dtheta_ contains the angular velocity of the shoulder (in column 0) and the elbow \n",
    "(in column 1) for each sample.  \n",
    "* _ddtheta_ contains the angular acceleration of the shoulder (in column 0) and the elbow \n",
    "(in column 1) for each sample. \n",
    "* _torque_ contains the torque of the shoulder (in column 0) and the elbow (in column 1) for each sample.  \n",
    "* _time_ contains the actual time stamp of each sample.\n",
    "\n",
    "A fold is a subset of the available data.  Each fold contains independent time points.\n",
    "\n",
    "### Objectives\n",
    "* Implement and understand __Holistic Cross Validation__\n",
    "* Perform a training set size sensitivity analysis and observe how hyper-parameter choices change with training set size\n",
    "\n",
    "### Instructions\n",
    "* Read the code below \n",
    "* For any cell that is flagged as *TODO*, complete the code according to the specifications\n",
    "* Execute each cell and verify that it is showing correct results\n",
    "* Hand-In Procedure\n",
    "  + Make sure that your notebook has been saved\n",
    "  + Download this file to your local machine (extension: .ipynb)\n",
    "  + Submit to the Gradscope Notebook HW06 dropbox\n",
    "\n",
    "\n",
    "### General References\n",
    "* [Guide to Jupyter](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook)\n",
    "* [Python Built-in Functions](https://docs.python.org/3/library/functions.html)\n",
    "* [Python Data Structures](https://docs.python.org/3/tutorial/datastructures.html)\n",
    "* [Numpy Reference](https://docs.scipy.org/doc/numpy/reference/index.html)\n",
    "* [Numpy Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)\n",
    "* [Summary of matplotlib](https://matplotlib.org/3.1.1/api/pyplot_summary.html)\n",
    "* [DataCamp: Matplotlib](https://www.datacamp.com/community/tutorials/matplotlib-tutorial-python?utm_source=adwords_ppc&utm_campaignid=1565261270&utm_adgroupid=67750485268&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=1t1&utm_creative=332661264365&utm_targetid=aud-299261629574:dsa-473406587955&utm_loc_interest_ms=&utm_loc_physical_ms=9026223&gclid=CjwKCAjw_uDsBRAMEiwAaFiHa8xhgCsO9wVcuZPGjAyVGTitb_-fxYtkBLkQ4E_GjSCZFVCqYCGkphoCjucQAvD_BwE)\n",
    "* [Pandas DataFrames](https://urldefense.proofpoint.com/v2/url?u=https-3A__pandas.pydata.org_pandas-2Ddocs_stable_reference_api_pandas.DataFrame.html&d=DwMD-g&c=qKdtBuuu6dQK9MsRUVJ2DPXW6oayO8fu4TfEHS8sGNk&r=9ngmsG8rSmDSS-O0b_V0gP-nN_33Vr52qbY3KXuDY5k&m=mcOOc8D0knaNNmmnTEo_F_WmT4j6_nUSL_yoPmGlLWQ&s=h7hQjqucR7tZyfZXxnoy3iitIr32YlrqiFyPATkW3lw&e=)\n",
    "* [Sci-kit Learn Linear Models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)\n",
    "* [Sci-kit Learn Ensemble Models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n",
    "* [Sci-kit Learn Metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "* [Sci-kit Learn Model Selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtEpd7j6d6rg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import scipy.stats as stats\n",
    "import os, re, fnmatch\n",
    "import pathlib, itertools, time\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import ElasticNet, Lasso\n",
    "\n",
    "# Default figure parameters\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['figure.constrained_layout.use'] = True\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-McDJZk8d6rj"
   },
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27_0NQCvd6rk",
    "outputId": "448cb63a-58dd-446d-82a2-7a74599751ba"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Load the BMI data from all the folds\n",
    "\"\"\"\n",
    "fname = '/mlp/datasets/bmi/bmi_dataset.pkl'\n",
    "\n",
    "with open(fname, 'rb') as f:\n",
    "    bmi = pkl.load(f)\n",
    "\n",
    "    MI_folds = bmi['MI'] \n",
    "    theta_folds = bmi['theta']\n",
    "    dtheta_folds = bmi['dtheta']\n",
    "    ddtheta_folds = bmi['ddtheta']\n",
    "    torque_folds = bmi['torque']\n",
    "    time_folds = bmi['time']\n",
    "\n",
    "\n",
    "nfolds = len(MI_folds)\n",
    "nfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B642OrHqd6ro",
    "outputId": "8b209a0e-c83c-4f0b-acda-7a4794f51f08"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED: EXECUTE CELL\n",
    "For this homework, it actually becomes rather difficult to work with \n",
    "individual arrays of different length, because numpy cannot efficiently\n",
    "concatenate them together  \n",
    "\n",
    "Here, we construct a single unified array for our data, which can \n",
    "then be indexed into using the folds_idx array. This allows us to efficiently \n",
    "index into our data instead of creating a copy of it each time we want to \n",
    "change the size of the the training set or our cross-validation rotation.\n",
    "\"\"\"\n",
    "\n",
    "# Starting index for each fold\n",
    "folds_idx = [None]*nfolds\n",
    "\n",
    "unified_idx = 0\n",
    "for i, fold in enumerate(time_folds):\n",
    "    # creates a list containing indexes from start of fold to end of fold,\n",
    "    # eg folds_idx[0] = [0,1,...,1192], folds_idx[1] = [1193,...,2296], ...\n",
    "    # we don't need to store all this (could just store start indexes),\n",
    "    # but this makes it a lot easier later\n",
    "    folds_idx[i] = list(range(unified_idx, unified_idx + fold.shape[0]))\n",
    "    unified_idx += fold.shape[0]\n",
    "\n",
    "def concat_folds(folds):\n",
    "    return np.concatenate(folds, axis=0)\n",
    "\n",
    "# These variables contain *ALL* of the data\n",
    "MI = concat_folds(MI_folds) \n",
    "theta = concat_folds(theta_folds)\n",
    "dtheta = concat_folds(dtheta_folds)\n",
    "ddtheta = concat_folds(ddtheta_folds)\n",
    "torque = concat_folds(torque_folds)\n",
    "time = concat_folds(time_folds)\n",
    "\n",
    "# Sizes of the entire data set\n",
    "print(MI.shape, theta.shape, dtheta.shape, ddtheta.shape, torque.shape, time.shape)\n",
    "\n",
    "# Starting index of each fold in the full data set\n",
    "[folds_idx[i][0] for i in range(nfolds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEQ9e1dLd6rp"
   },
   "source": [
    "# PARAMETER SET LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iA0CcLFSd6rr"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED: EXECUTE CELL\n",
    "\n",
    "Construct the Cartesian product of the provided hyper-parameters.\n",
    "\n",
    "This implementation is similar to what is used by GridSearchCV to create its\n",
    "Cartesian product of hyper-parameters.\n",
    "\n",
    "Note that one can also include hyper-parameters in the list with only a single\n",
    "option.  This has the effect of simply setting the same hyper-parameter value for each\n",
    "of resulting hyper-parameter sets.\n",
    "\n",
    "\"\"\"\n",
    "def generate_paramsets(param_lists):\n",
    "    '''\n",
    "    Construct the Cartesian product of the parameters\n",
    "    PARAMS:\n",
    "        params_lists: dict of lists of values to try for each parameter.\n",
    "                      keys of the dict are the names of the parameters\n",
    "                      values are lists of values to try for the \n",
    "                      corresponding parameter\n",
    "    RETURNS: a list of dicts of hyper-parameter sets.  These make up the \n",
    "    Cartesian product of the possible hyper-parameters\n",
    "    '''\n",
    "    keys, values = zip(*param_lists.items())\n",
    "    \n",
    "    # Determines Cartesian product of parameter values\n",
    "    combos = itertools.product(*values)\n",
    "    \n",
    "    # Constructs list of dictionaries\n",
    "    combos_dicts = [dict(zip(keys, vals)) for vals in combos]\n",
    "    return list(combos_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NskWaATd6rs"
   },
   "source": [
    "# PERFORMANCE EVALUTION\n",
    "Tools for evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmGUQ1R_d6rt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED: EXECUTE CELL\n",
    "Evaluate the training performance of an already trained model\n",
    "\"\"\"\n",
    "def mse_rmse(trues, preds):\n",
    "    '''\n",
    "    Compute MSE and rMSE for each column separately.\n",
    "    '''\n",
    "    mse = np.sum(np.square(trues - preds), axis=0) / trues.shape[0]\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mse, rmse, rmse * 180 / np.pi\n",
    "\n",
    "def predict_score_eval(model, X, y):\n",
    "    '''\n",
    "    Compute the model predictions and cooresponding scores.\n",
    "    PARAMS:\n",
    "        model: the trained model used to make predicitons\n",
    "        X: feature data (MxN)\n",
    "        y: cooresponding output (Mxk)\n",
    "    RETURNS: Dictionary that contains:\n",
    "        mse: mean squared error for each column (k vector)\n",
    "        rmse: rMSE in radians (k vector)\n",
    "        rmse_deg: rMSE in degrees (k vector).  Note that this will only make sense\n",
    "           if rmse is in radians\n",
    "        fvaf: fraction of variance accounted for metric (k vector)\n",
    "    '''\n",
    "    preds = model.predict(X) \n",
    "    \n",
    "    # Fraction of Variance Accounted For\n",
    "    fvaf = model.score(X, y)\n",
    "    \n",
    "    # All other metrics\n",
    "    mse, rmse, rmse_deg = mse_rmse(y, preds) \n",
    "\n",
    "    results = {\n",
    "        'mse'  : np.reshape(mse,  (1, -1)), \n",
    "        'rmse' : np.reshape(rmse, (1, -1)), \n",
    "        'rmse_deg' : np.reshape(rmse_deg, (1, -1)), \n",
    "        'fvaf' : np.reshape(fvaf, (1, -1)),  # Fraction of Variance Accounted For\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YS0S51F7d6ru"
   },
   "source": [
    "# CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBwCLg43d6rv"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Complete K-Fold Cross-Validation implementation\n",
    "\n",
    "For a given model and training set size, this class will perform a loop over rotations\n",
    "\n",
    "For each combination, the model will be independently trained and evaluated on \n",
    "the training, validation and testing sets.  Evaluation for each model will be done by a\n",
    "specified evaluation function (eval_func), which returns a dictionary of metric values.\n",
    "These results are collated across the rotations.  The return structures include the \n",
    "performance for each rotation, as well as the mean and standard deviation across the rotations.\n",
    "\n",
    "'''\n",
    "class KFoldHolisticCrossValidation():\n",
    "    '''\n",
    "    Cross-validation class. This class will perform cross-validation across for a \n",
    "    single hyper-parameter set.\n",
    "    '''\n",
    "    def __init__(self, model, eval_func, rotation_skip=1):\n",
    "        '''\n",
    "        :param model: The Scikit-Learn model to be trained\n",
    "        :param eval_func': Python function that will be used to evaluate a model\n",
    "                                parameters: (inputs, desired outputs, model predictions)\n",
    "        :param rotation_skip: Number of CV rotations for every one rotation that is actually trained & evaluated.  \n",
    "                                Typical is 1 (train and evaluate all rotations), but when we are \n",
    "                                debugging, it is helpful to perform a smaller number of train/evaluate\n",
    "                                cycles.\n",
    "        '''\n",
    "        # TODO: set the class variables\n",
    "        self.model = #TODO\n",
    "        self.eval_func = #TODO\n",
    "        self.rotation_skip = #TODO\n",
    "\n",
    "    def perform_cross_validation(self, X, y, folds_idx, trainsize):\n",
    "        ''' TODO: This is where the bulk of the work will be done\n",
    "        Perform cross validation for a singular train set size and single \n",
    "        hyper-parameter set, by evaluating the model's performance over \n",
    "        multiple data set rotations.\n",
    "\n",
    "        NOTE: This function assumes the hyper-parameters have already been \n",
    "              set in the model\n",
    "            \n",
    "        PARAMS:\n",
    "            X: numpy array containing all of the input data \n",
    "               (folds concatenated together)\n",
    "            y: numpy array containing all of the desired output data \n",
    "               (folds concatenated together)\n",
    "            folds_idx: list of lists containing the indexes\n",
    "                       of each element in each fold, eg\n",
    "                       folds_idx[i] = [start_idx, ... , end_idx]\n",
    "            trainsize: number of folds to use for training\n",
    "            \n",
    "        RETURNS: train, val, and test set results for all rotations of the  \n",
    "                 data sets and the summary  of the results (i.e., the \n",
    "                 mean/standard devation over all the rotations). \n",
    "                 \n",
    "                 results is a dictionary of dictionaries of r-by-n numpy \n",
    "                 arrays, where r is the number of rotations, and n is the \n",
    "                 number of outputs from the model.\n",
    "                 \n",
    "                 summary is a dict of dictionaries of 1-by-n numpy arrays containing\n",
    "                 the mean and standard deviation of the metrics in results across\n",
    "                 all rotations\n",
    "                 \n",
    "                 In our BMI dataset, n = 2 (e.g., shoulder torque and elbow torque)\n",
    "\n",
    "                 General form:\n",
    "                     results.keys() = ['train', 'val', 'test']\n",
    "\n",
    "                     results['train'].keys() = ['metric1', 'metric2', ...]\n",
    "                         where metrics are defined by the dictionary returned by the eval_func\n",
    "                     \n",
    "                     results['train']['metric1'] = numpy_array\n",
    "                     \n",
    "                     results = \n",
    "                     {\n",
    "                        'train':\n",
    "                                 {\n",
    "                                     'mse' : r_by_n_numpy_array,\n",
    "                                     'rmse': r_by_n_numpy_array, \n",
    "                                     ...\n",
    "                                 },\n",
    "                        'val'  : {...},\n",
    "                        'test' : {...}\n",
    "                     }\n",
    "                     \n",
    "                     summary = \n",
    "                     {\n",
    "                        'train':\n",
    "                                 {\n",
    "                                     'mse_mean' : 1_by_n_numpy_array,\n",
    "                                     'mse_std'  : 1_by_n_numpy_array,\n",
    "                                     'rmse_mean': 1_by_n_numpy_array, \n",
    "                                     'rmse_std' : 1_by_n_numpy_array,\n",
    "                                     ...\n",
    "                                 },\n",
    "                        'val'  : {...},\n",
    "                        'test' : {...}\n",
    "                     }\n",
    "\n",
    "                    For example, you can access the MSE results for the \n",
    "                    validation set like so:\n",
    "                        results['val']['mse'] \n",
    "                    For example, you can access the summary (i.e. the average \n",
    "                    results over all the rotations) for the test set for the\n",
    "                    rMSE like so:\n",
    "                        summary['test']['rmse_mean']                \n",
    "        '''\n",
    "        \n",
    "        # Verify that a valid train set size was provided\n",
    "        nfolds = len(folds_idx)\n",
    "        if trainsize > nfolds - 2: \n",
    "            err_msg = \"ERROR: KFoldHolisticCrossValidation.perform_cross_validation() - \"\n",
    "            err_msg += \"trainsize (%d) cant be more than nfolds (%d) - 2\" % (trainsize, nfolds)\n",
    "            raise ValueError(err_msg)\n",
    "        \n",
    "        # Set up results data structures for each rotation\n",
    "        results = {'train': None, 'val': None, 'test': None}\n",
    "        summary = {'train': {}, 'val': {}, 'test': {}}\n",
    "        \n",
    "        model = self.model\n",
    "        evaluate = self.eval_func\n",
    "        \n",
    "        # Rotate through different train, val, and test sets\n",
    "        for rotation in range(0, nfolds, self.rotation_skip):\n",
    "            (\n",
    "                Xtrain, ytrain, Xval, yval,  Xtest, ytest\n",
    "            ) = self.get_data(X, y, folds_idx, nfolds, rotation, trainsize)\n",
    "\n",
    "            print('Rotation:', rotation, '; train examples:', ytrain.shape)\n",
    "            \n",
    "            # TODO: Train model using the training set\n",
    "\n",
    "            # TODO: Evaluate the model for each set\n",
    "            res_train =  #TODO\n",
    "            res_val =  #TODO\n",
    "            res_test =  #TODO\n",
    "\n",
    "            # Record the train, val, and test set results. These are dicts \n",
    "            # of result metrics, returned by the evaluate function\n",
    "\n",
    "            if results['train'] is None: \n",
    "                # First rotation: initialize structures\n",
    "                results['train'] = res_train\n",
    "                results['val'] =  #TODO\n",
    "                results['test'] = #TODO\n",
    "            else:\n",
    "                # Other rotations: add results to existing structures\n",
    "                for metric in res_train.keys():\n",
    "                    results['train'][metric] = np.append(results['train'][metric], res_train[metric], axis=0)\n",
    "                    results['val'][metric] = np.append(results['val'][metric], res_val[metric], axis=0)\n",
    "                    results['test'][metric] = np.append(results['test'][metric], res_test[metric], axis=0)\n",
    "\n",
    "        # Compute/record mean and standard deviation for the size for each metric\n",
    "        #  The mean is across the rotations\n",
    "        for metric in results['train'].keys():\n",
    "            for stat_set in ['train', 'val', 'test']:\n",
    "                summary[stat_set][metric+'_mean'] = np.mean(results[stat_set][metric], \n",
    "                                                            axis=0).reshape(1, -1)\n",
    "                summary[stat_set][metric+'_std'] = np.std(results[stat_set][metric], \n",
    "                                                          axis=0).reshape(1, -1)\n",
    "\n",
    "        return results, summary\n",
    "\n",
    "    def get_data(self, X, y, folds_idx, nfolds, rotation, trainsize):\n",
    "        '''TODO\n",
    "        Determines the fold indices for the train, val, and test set given\n",
    "        the total number of folds, rotation, and training set size.\n",
    "        Use these fold indices to get the training, validation, and test sets\n",
    "        from all_xfolds and all_folds\n",
    "        '''\n",
    "        # Determine folds to use \n",
    "        # (eg fold 1,2,3 for trainsize=3, rotation=1, nfolds=20)\n",
    "        trainfolds = (np.arange(trainsize) + rotation) % nfolds\n",
    "        # Single fold for validation\n",
    "        valfold = (nfolds - 2 + rotation) % nfolds\n",
    "        # Single fold for testing\n",
    "        testfold = (valfold + 1) % nfolds\n",
    "        \n",
    "        # Construct a list to serve as an index into X for our training\n",
    "        # samples. This will contain the index of each sample the training set\n",
    "        train_idx = []\n",
    "        for i in trainfolds:\n",
    "            # the + operator concatenates raw python lists\n",
    "            train_idx += folds_idx[i] \n",
    "\n",
    "        # TODO: Construct train set by indexing into X and y with the indices of the\n",
    "        #  samples that belong to the training set\n",
    "        Xtrain = X[train_idx]\n",
    "        ytrain = #TODO\n",
    "        \n",
    "        # TODO: Construct validation set using the valfold (a single fold!)\n",
    "        #       Hint: this is always one fold\n",
    "        Xval =  #TODO\n",
    "        yval =  #TODO\n",
    "\n",
    "        # TODO: Construct test set using the testfold (a single fold!)\n",
    "        Xtest = #TODO\n",
    "        ytest = #TODO\n",
    "        \n",
    "        return Xtrain, ytrain, Xval, yval, Xtest, ytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzmIoQXcd6rw"
   },
   "outputs": [],
   "source": [
    "class CrossValidationGridSearch():\n",
    "    '''\n",
    "    This class is responsible for performing a grid trainsizes x paramsets CV experiments.\n",
    "    For each grid point, N-fold crossvalidation is performed (with potential skips in the\n",
    "    possible rotations).\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, model, paramsets, eval_func, opt_metric, \n",
    "                 maximize_opt_metric=False, trainsizes=[1], rotation_skip=1,\n",
    "                ):\n",
    "        ''' TODO\n",
    "        Class instance constructor\n",
    "        \n",
    "        :param model: Model to be learned\n",
    "        :param paramsets: List of dicts.  Every dict contains a set of hyper-parameters for use in\n",
    "                            one experiment\n",
    "        :param eval_func: Python function that will be used to evaluate a model\n",
    "                                parameters: (inputs, desired outputs, model predictions)\n",
    "        :param opt_metric: Optimization metric to be used\n",
    "        :param maximize_opt_metric: True -> best model has high value for performance metric; \n",
    "                                    False -> best model has low value\n",
    "        :param trainsizes: A list of training set sizes (in terms of number of folds)\n",
    "        :param rotation_skip: Number of CV rotations for every one rotation to train & evaluate.  \n",
    "                                Typical is 1 (train and evaluate all rotations), but when we are \n",
    "                                debugging, it is helpful to perform a smaller number of train/evaluate\n",
    "                                cycles\n",
    "        '''\n",
    "        # TODO: set the class variables\n",
    "        self.model =  #TODO\n",
    "        self.paramsets =  #TODO\n",
    "        self.trainsizes =  #TODO\n",
    "        self.eval_func =  #TODO\n",
    "        self.opt_metric = opt_metric + '_mean' \n",
    "        self.maximize_opt_metric =  #TODO\n",
    "        self.rotation_skip = #TODO\n",
    "        \n",
    "        # Results attributes\n",
    "        # Full recording of all results for all paramsets, sizes, rotations,\n",
    "        # and metrics. This is a list of dictionaries for each paramset\n",
    "        self.results = []\n",
    "        # Validation summary report of all means and standard deviations for \n",
    "        # all metrics, for all paramsets, and sizes. This is a 3D s-by-r-by-p \n",
    "        # numpy array. Where s is the number of sizes, r the number of summary \n",
    "        # metrics +2, and p is the number of paramsets\n",
    "        self.report_by_size = None\n",
    "        # List of the indices of the best paramset for each size\n",
    "        self.best_param_inds = None\n",
    "\n",
    "    def load_checkpoint(self, fname):\n",
    "        ''' PROVIDED\n",
    "        Load a checkpoint file into self.results\n",
    "        \n",
    "        :param fname: Full name of the file to load the checkpoint from. \n",
    "        '''\n",
    "        if not os.path.exists(fname):\n",
    "            raise ValueError('File %s does not exist'%fname)\n",
    "        \n",
    "        with open(fname, 'rb') as f:\n",
    "            self.results = pkl.load(f)\n",
    "            \n",
    "    def dump_checkpoint(self, fname):\n",
    "        ''' PROVIDED\n",
    "        Write the current set of results to a checkpoint file\n",
    "        \n",
    "        :param fname: Full name of file to write checkpoint to\n",
    "        '''\n",
    "        with open(fname, 'wb') as f:\n",
    "            pkl.dump(self.results, f)\n",
    "            \n",
    "    def reset_results(self):\n",
    "        ''' PROVIDED\n",
    "        Reset the current set of results that are stored internally\n",
    "        '''\n",
    "        self.results = []\n",
    "    \n",
    "    def cross_validation_gridsearch(self, X, y, folds_idx, checkpoint_fname=None):\n",
    "        ''' TODO\n",
    "        Perform the grid search with the given data (X, y, folds_idx).  This grid search is\n",
    "        smart in that if a specific set of hyper-parameters have already been done (encoded in\n",
    "        the checkpoint), then building and evaluating for those hyper-parameters will not be done \n",
    "        again.\n",
    "        \n",
    "        :param X: Full set of input features\n",
    "        :param y: Full set of desired outputs\n",
    "        :param folds_idx: List of row indices in X/y, one for each fold\n",
    "        :param checkpoint_fname: Name of the output checkpoint file.  If None, then not written\n",
    "                to file.\n",
    "        '''\n",
    "\n",
    "        # Create the cross-validation instance\n",
    "        cross_val = KFoldHolisticCrossValidation(\n",
    "            self.model, self.eval_func, \n",
    "            rotation_skip = self.rotation_skip\n",
    "        )        \n",
    "        \n",
    "        # Try to load the checkpoint file\n",
    "        if checkpoint_fname is not None and os.path.exists(checkpoint_fname):\n",
    "            self.load_checkpoint(checkpoint_fname)\n",
    "\n",
    "        # Iterate over the parameter sets\n",
    "        for params in self.paramsets:\n",
    "\n",
    "            # Check that we haven't already done this before (from our checkpoint)\n",
    "            if params in [r['params'] for r in self.results]:\n",
    "                print('already evaled:', params)\n",
    "                continue\n",
    "      \n",
    "            print('evaling on:', params)\n",
    "        \n",
    "            # Set up the results for these parametrs\n",
    "            param_results = []\n",
    "            param_summary = None\n",
    "\n",
    "            # Set the parameters in the model\n",
    "            self.model.set_params(**params)\n",
    "            \n",
    "            # Iterate over the different train set sizes\n",
    "            # Running cross-validation on thm\n",
    "            for size in self.trainsizes:\n",
    "                # TODO: Perform Cross-Validation\n",
    "                result, summary = # TODO\n",
    "                \n",
    "                # Append results in param_results\n",
    "                param_results.append(result)\n",
    "                \n",
    "                # Append the mean and standard deviation statistics (summary)\n",
    "                if param_summary is None: \n",
    "                    param_summary = summary\n",
    "                else:\n",
    "                    # For each metric measured, append the summary results\n",
    "                    for metric in summary['train'].keys():\n",
    "                        for stat_set in ['train', 'val', 'test']:\n",
    "                            param_summary[stat_set][metric] = np.append(\n",
    "                                param_summary[stat_set][metric], \n",
    "                                summary[stat_set][metric], \n",
    "                                axis=0\n",
    "                            )\n",
    "                            \n",
    "            # Add this param's results to this accumulating results\n",
    "            self.results.append({\n",
    "                'params' :params,\n",
    "                'results':param_results, \n",
    "                'summary':param_summary\n",
    "            })\n",
    "\n",
    "            # Write the checkpoint file\n",
    "            if checkpoint_fname is not None:\n",
    "                self.dump_checkpoint(checkpoint_fname)\n",
    "        \n",
    "\n",
    "    def get_reports_all(self):\n",
    "        ''' PROVIDED\n",
    "        Generate reports on the internally stored results\n",
    "        \n",
    "        :return: Dictionary containing two keys: 'report_by_size', 'best_param_inds'\n",
    "        '''\n",
    "        self.report_by_size = self.get_reports()\n",
    "        self.best_param_inds = self.get_best_params(\n",
    "            self.opt_metric, self.maximize_opt_metric\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'report_by_size' : self.report_by_size, \n",
    "            'best_param_inds': self.best_param_inds\n",
    "        }\n",
    "    \n",
    "    # Report generation code. Provided, but you should still read it\n",
    "    \"\"\" PROVIDED\n",
    "    Functions to generate a report of the result of the cross-validation r5un\n",
    "    \"\"\"\n",
    "    def get_reports(self):\n",
    "        ''' PROVIDED\n",
    "        Get the mean validation summary of all the parameters for each size\n",
    "        for all metrics. This is used to determine the best parameter set  \n",
    "        for each size\n",
    "        \n",
    "        RETURNS: the report_by_size as a 3D s-by-r-by-p array. Where s is \n",
    "                 the number of train sizes tried, r is the number of summary  \n",
    "                 metrics evaluated+2, and p is the number of parameter sets.\n",
    "        '''\n",
    "        results = self.results\n",
    "        sizes = np.reshape(self.trainsizes, (1, -1))\n",
    "        \n",
    "        nsizes = sizes.shape[1]\n",
    "        nparams = len(results)\n",
    "        \n",
    "        # Set up the reports objects\n",
    "        metrics = list(results[0]['summary']['val'].keys())\n",
    "        colnames = ['params', 'size'] + metrics \n",
    "        report_by_size = np.empty((nsizes, len(colnames), nparams), dtype=object)\n",
    "\n",
    "        # Determine mean val for each paramset for each size for all metrics\n",
    "        for p, paramset_result in enumerate(results):\n",
    "            params = paramset_result['params']\n",
    "            res_val = paramset_result['summary']['val']\n",
    "\n",
    "            # Compute mean val result for each train size for each metric\n",
    "            means_by_size = [np.mean(res_val[metric], axis=1) \n",
    "                             for metric in metrics]\n",
    "            \n",
    "            print(\"MEAN\", means_by_size)\n",
    "            # Include the train set sizes into the report\n",
    "            means_by_size = np.append(sizes, means_by_size, axis=0)\n",
    "            print(\"SIZES\", sizes)\n",
    "            print(\"MEAN2\", means_by_size)\n",
    "            # Include the parameter sets into the report\n",
    "            param_strgs = np.reshape([str(params)]*nsizes, (1, -1))\n",
    "            means_by_size = np.append(param_strgs, means_by_size, axis=0).T\n",
    "            print(\"MEAN3\", means_by_size)\n",
    "            # Append the parameter set means into the report \n",
    "            report_by_size[:,:,p] = means_by_size\n",
    "        return report_by_size\n",
    "\n",
    "    def get_best_params(self, opt_metric, maximize_opt_metric):\n",
    "        ''' PROVIDED (Do read through all the provided code)\n",
    "        Determines the best parameter set for each train size,  \n",
    "        based on a specific metric.\n",
    "        \n",
    "        PARAMS:\n",
    "            opt_metric: optimized metric. one of the metrics returned \n",
    "                        from eval_func, with '_mean' appended for the\n",
    "                        summary stat. This is the mean metric used to  \n",
    "                        determine the best parameter set for each size\n",
    "                        \n",
    "            maximize_opt_metric: True if the max of opt_metric should be\n",
    "                                 used to determine the best parameters.\n",
    "                                 False if the min should be used.\n",
    "        RETURNS: list of best parameter set indicies for each size \n",
    "        '''\n",
    "        results = self.results\n",
    "        report_by_size = self.report_by_size \n",
    "                \n",
    "        metrics = list(results[0]['summary']['val'].keys())\n",
    "        \n",
    "        # Determine best params for each size, for the optimized metric\n",
    "        best_param_inds = None\n",
    "        metric_idx = metrics.index(opt_metric)\n",
    "        \n",
    "        # Report info for all paramsets for the optimized metric\n",
    "        report_opt_metric = report_by_size[:, metric_idx+2, :]\n",
    "        \n",
    "        if maximize_opt_metric:\n",
    "            # Add two for the additional cols for params and size\n",
    "            best_param_inds = np.argmax(report_opt_metric, axis=1)\n",
    "        else: \n",
    "            best_param_inds = np.argmin(report_opt_metric, axis=1)\n",
    "        # Return list of best params indices for each size\n",
    "        return best_param_inds\n",
    "    \n",
    "    def get_best_params_strings(self):\n",
    "        ''' PROVIDED\n",
    "        Generates a list of strings of the best params for each size\n",
    "        RETURNS: list of strings of the best params for each size\n",
    "        '''\n",
    "        best_param_inds = self.best_param_inds\n",
    "        results = self.results\n",
    "        return [str(results[p]['params']) for p in best_param_inds]\n",
    "\n",
    "    def get_report_best_params_for_size(self, size):\n",
    "        ''' PROVIDED\n",
    "        Get the mean validation summary for the best parameter set \n",
    "        for a specific size for all metrics.\n",
    "        PARAMS:\n",
    "            size: index of desired train set size for the best  \n",
    "                  paramset to come from. Size here is the index in \n",
    "                  the trainsizes list, NOT the actual number of folds.\n",
    "        RETURNS: the best parameter report for the size as an s-by-m  \n",
    "                 dataframe. Where each row is for a different size, and \n",
    "                 each column is for a different summary metric.\n",
    "        '''\n",
    "        best_param_inds = self.best_param_inds\n",
    "        report_by_size = self.report_by_size \n",
    "        \n",
    "        bp_index = best_param_inds[size]\n",
    "        size_len = len(size) if type(size) is list else 1\n",
    "                \n",
    "        metrics = list(self.results[0]['summary']['val'].keys())\n",
    "        colnames = ['params', 'size'] + metrics\n",
    "        report_best_params_for_size = pd.DataFrame(\n",
    "            report_by_size[size_idx].T[bp_index].reshape(size_len,-1),\n",
    "            columns=colnames\n",
    "        )\n",
    "        return report_best_params_for_size\n",
    "\n",
    "    \"\"\" PROVIDED\n",
    "    Plotting code to display the result of the gird search and cross-validation\n",
    "    \"\"\"\n",
    "\n",
    "    def plot_cv(self, foldsindices, results, summary, metrics, size):\n",
    "        ''' PROVIDED\n",
    "        Plotting function for after perform_cross_validation(), \n",
    "        displaying the train and val set performances for each rotation \n",
    "        of the training set. \n",
    "        \n",
    "        PARAMS:\n",
    "            foldsindices: indices of the train sets tried\n",
    "            results: results from perform_cross_validation()\n",
    "            summary: mean and standard deviations of the results\n",
    "            metrics: list of result metrics to plot. Available metrics \n",
    "                     are the keys in the dict returned by eval_func\n",
    "            size: train set size\n",
    "            \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        nmetrics = len(metrics)\n",
    "\n",
    "        # Initialize figure plots\n",
    "        fig, axs = plt.subplots(nmetrics, 1, figsize=(12,6))\n",
    "        fig.subplots_adjust(hspace=.4)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "\n",
    "        # Construct each subplot\n",
    "        for metric, ax in zip(metrics, axs):\n",
    "            # Compute the mean for multiple outputs\n",
    "            res_train = np.mean(results['train'][metric], axis=1)\n",
    "            res_val = np.mean(results['val'][metric], axis=1)\n",
    "            \n",
    "            # Plot\n",
    "            ax.plot(foldsindices, res_train, label='train')\n",
    "            ax.plot(foldsindices, res_val, label='val')\n",
    "            #ax.plot(foldsindices, res_test, label='test')\n",
    "            ax.set(ylabel=metric)\n",
    "        axs[0].legend(loc='upper right')\n",
    "        axs[0].set(xlabel='Fold Index')\n",
    "        axs[0].set(title='Performance for Train Set Size ' + str(size))\n",
    "        return fig, axs\n",
    "\n",
    "    def plot_param_train_val(self, metrics, paramidx=0, view_test=False):\n",
    "        ''' PROVIDED\n",
    "        Plotting function for after grid_cross_validation(), \n",
    "        displaying the mean (summary) train and val set performances \n",
    "        for each train set size.\n",
    "        \n",
    "        PARAMS:\n",
    "            metrics: list of summary metrics to plot. '_mean' or '_std'\n",
    "                     must be appended to the end of the base metric name. \n",
    "                     These base metric names are the keys in the dict \n",
    "                     returned by eval_func\n",
    "            paramidx: parameter set index\n",
    "            view_test: flag to view the test set results\n",
    "            \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        sizes = self.trainsizes\n",
    "        results = self.results\n",
    "\n",
    "        summary = results[paramidx]['summary']\n",
    "        params = results[paramidx]['params']\n",
    "        \n",
    "        nmetrics = len(metrics)\n",
    "\n",
    "        # Initialize figure plots\n",
    "        fig, axs = plt.subplots(nmetrics, 1, figsize=(12,6))\n",
    "        \n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "\n",
    "        # Construct each subplot\n",
    "        for metric, ax in zip(metrics, axs):\n",
    "            # Compute the mean for multiple outputs\n",
    "            res_train = np.mean(summary['train'][metric], axis=1)\n",
    "            res_val = np.mean(summary['val'][metric], axis=1)\n",
    "            \n",
    "            # Plot\n",
    "            ax.plot(sizes, res_train, label='train')\n",
    "            ax.plot(sizes, res_val, label='val')\n",
    "            if view_test:\n",
    "                res_test = np.mean(summary['test'][metric], axis=1)\n",
    "                ax.plot(sizes, res_test, label='test')\n",
    "            ax.set(ylabel=metric)\n",
    "            ax.set_xticks(sizes)\n",
    "            \n",
    "        # Final labels\n",
    "        axs[-1].set(xlabel='Train Set Size (# of folds)')\n",
    "        axs[0].set(title=str(params))\n",
    "        axs[0].legend(loc='upper right')\n",
    "        return fig, axs\n",
    "    \n",
    "    def plot_allparams_val(self, metrics):\n",
    "        ''' PROVIDED\n",
    "        Plotting function for after grid_cross_validation(), displaying  \n",
    "        mean (summary) validation set performances for each train size \n",
    "        for all parameter sets for the specified metrics.\n",
    "        \n",
    "        PARAMS:\n",
    "            metrics: list of summary metrics to plot. '_mean' or '_std' \n",
    "                     must be append to the end of the base metric name. \n",
    "                     These base metric names are the keys in the dict \n",
    "                     returned by eval_func\n",
    "                     \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        sizes = self.trainsizes\n",
    "        results = self.results\n",
    "        \n",
    "        nmetrics = len(metrics)\n",
    "\n",
    "        # Initialize figure plots\n",
    "        fig, axs = plt.subplots(nmetrics, 1, figsize=(10,6))\n",
    "        #fig.subplots_adjust(hspace=.4)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "\n",
    "        # Construct each subplot: one for each metric\n",
    "        for metric, ax in zip(metrics, axs):\n",
    "            \n",
    "            # Iterate over the hyper-parameter sets\n",
    "            for p, param_results in enumerate(results):\n",
    "                summary = param_results['summary']\n",
    "                params = param_results['params']\n",
    "                # Compute the mean for multiple outputs\n",
    "                res_val = np.mean(summary['val'][metric], axis=1)                \n",
    "                ax.plot(sizes, res_val, label=str(params))\n",
    "                \n",
    "            # Labels for this metric\n",
    "            ax.set(ylabel=metric)\n",
    "            ax.set_xticks(sizes)\n",
    "            \n",
    "        # Final labels\n",
    "        axs[-1].set(xlabel='Train Set Size (# of folds)')\n",
    "        axs[0].set(title='Validation Performance')\n",
    "        axs[0].legend(bbox_to_anchor=(1.02, 1), loc='upper left',\n",
    "                      ncol=1, borderaxespad=0., prop={'size': 8})\n",
    "        return fig, axs\n",
    "\n",
    "    def plot_best_params_by_size(self):\n",
    "        ''' PROVIDED\n",
    "        Plotting function for after grid_cross_validation(), displaying \n",
    "        mean (summary) train and validation set performances for the best \n",
    "        parameter set for each train size for the optimized metric.\n",
    "                     \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        results = self.results\n",
    "        metric = self.opt_metric\n",
    "        best_param_inds = self.best_param_inds\n",
    "        sizes = np.array(self.trainsizes)\n",
    "\n",
    "        # Unique set of best params for the legend\n",
    "        unique_param_sets = np.unique(best_param_inds)\n",
    "        lgnd_params = [self.paramsets[p] for p in unique_param_sets]\n",
    "\n",
    "        # Data set types to display\n",
    "        set_names = ['train', 'val', 'test']\n",
    "        \n",
    "        # Initialize figure\n",
    "        fig, axs = plt.subplots(len(set_names), 1, figsize=(10,8))\n",
    "\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "        \n",
    "        # Construct each subplot: iterate over data set types (train and val only)\n",
    "        for i, (ax, set_name) in enumerate(zip(axs, set_names)):\n",
    "            \n",
    "            # Iterate over the unique set of hyperparameters\n",
    "            for p in unique_param_sets:\n",
    "                # Obtain indices of sizes this paramset was best for\n",
    "                param_size_inds = np.where(best_param_inds == p)[0]\n",
    "                param_sizes = sizes[param_size_inds]\n",
    "                param_sizes.sort()\n",
    "                # Compute the mean over multiple outputs for each size\n",
    "                param_summary = results[p]['summary'][set_name]\n",
    "                metric_scores = np.mean(param_summary[metric][param_size_inds,:], axis=1)\n",
    "                # Plot the param results for each size it was the best for\n",
    "                ax.scatter(param_sizes, metric_scores, s=120, marker=(p+2, 1))\n",
    "                \n",
    "            # Ticks for all data set sizes\n",
    "            ax.set_xticks(sizes)\n",
    "\n",
    "            set_name += ' Set Performance'\n",
    "            ax.set(ylabel=metric, title=set_name)\n",
    "\n",
    "        # Final labels\n",
    "        axs[-1].set(xlabel='Train Set Size (# of folds)')\n",
    "        axs[0].legend(lgnd_params, bbox_to_anchor=(1.02, 1), loc='upper left',\n",
    "                      ncol=1, borderaxespad=0., prop={'size': 8})\n",
    "        \n",
    "        return fig, axs\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R498w0V0d6ry"
   },
   "source": [
    "# PERFORM CROSS VALIDATION FOR LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFPq3LXBd6rz",
    "outputId": "5d838b88-56cf-47e3-bed1-3f39cbc0fe2c"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED: EXECUTE CELL\n",
    "Generate list of parameters to use for cross-validation\n",
    "using generate_paramsets()\n",
    "\n",
    "Create a dictionary that contains parameter lists for alpha and max_iter\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# For testing purposes, you may want to make the parameter lists smaller\n",
    "\n",
    "# param_lists = #TODO\n",
    "param_lists = {\n",
    "    'alpha': np.logspace(-5, -1, base=10, num=10, endpoint=True),\n",
    "    'max_iter':[1000],\n",
    "    'tol':[1e-3]\n",
    "} \n",
    "    #, 'warm_start':[False]\n",
    "\n",
    "allparamsets = generate_paramsets(param_lists) #TODO\n",
    "#allparamsets = # TODO\n",
    "allparamsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtIulw5_d6r1"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Initialize the cross validation object. Use Lasso for the \n",
    "model, use train set sizes of 1,2,3,4,6,9 use fvaf\n",
    "as the metric to optimize, and 1 for the skip. We want to  \n",
    "select the best fvaf \n",
    "\"\"\"\n",
    "model = Lasso()\n",
    "trainsizes = #TODO\n",
    "opt_metric = #TODO\n",
    "maximize_opt_metric = #TODO\n",
    "#skip = 10 # For testing only\n",
    "skip = 1\n",
    "\n",
    "# Create the cross validation grid search object\n",
    "crossval = CrossValidationGridSearch(\n",
    "    model, allparamsets, predict_score_eval, opt_metric,\n",
    "    maximize_opt_metric, trainsizes, skip\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icL9zlq5d6r1"
   },
   "outputs": [],
   "source": [
    "''' PROVIDED: EXECUTE CELL\n",
    "\n",
    "Checkpoint file: for a given gridsearch, we checkpoint the individual training experiments to\n",
    "a file.  This way, if the search is interrupted, we can restart the search where it left off.  \n",
    "Also - we can add more hyper-parameter sets later and restart the search.\n",
    "\n",
    "'''\n",
    "\n",
    "# if you are using Jupyter, then you may want to store locally:\n",
    "fullcvfname = \"hw6_crossval_checkpoint.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSJ706lkd6r2",
    "outputId": "22a5e000-00a4-422d-9425-52409bef3370",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Execute the grid_cross_validation() procedure for all \n",
    "parameters and sizes\n",
    "\n",
    "\"\"\"\n",
    "# TODO: make sure this is set appropriately. \"True\" if you want to \n",
    "#       just always to run cross validation, \"False\" if you want \n",
    "#       to re-load results from a previous run\n",
    "# If you change the parameters but do not set force = True,\n",
    "# the model will not be re-run and your previously selected\n",
    "# parameters will be used\n",
    "force = False\n",
    "#force = True\n",
    "\n",
    "if force and os.path.exists(fullcvfname):\n",
    "    # Delete the checkpoint file\n",
    "    os.remove(fullcvfname)\n",
    "    \n",
    "# TODO: Use grid_cross_validation() to run the full cross- \n",
    "#       validation procedure\n",
    "# Note: when testing, run this using small lists of parameters \n",
    "#       (e.g. of length 2 or 4) and/or small trainsize lists \n",
    "#       (e.g. [1, 2, 3, 4, 5])\n",
    "# Note: for the final submission, make sure to use the complete \n",
    "#       parameter set list and trainsize list provided/specified\n",
    "#       This will take some time (longer than an hour)\n",
    "    \n",
    "crossval.cross_validation_gridsearch(\n",
    "        MI, torque, folds_idx,\n",
    "        fullcvfname \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSJ706lkd6r2",
    "outputId": "22a5e000-00a4-422d-9425-52409bef3370",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PROVIDED: EXECUTE CELL\n",
    "# Get the report from the training process\n",
    "crossval_report = crossval.get_reports_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSJ706lkd6r2",
    "outputId": "22a5e000-00a4-422d-9425-52409bef3370",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PROVIDED: EXECUTE CELL\n",
    "crossval_report.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSJ706lkd6r2",
    "outputId": "22a5e000-00a4-422d-9425-52409bef3370"
   },
   "outputs": [],
   "source": [
    "# PROVIDED: EXECUTE CELL\n",
    "\n",
    "# Summary results for validation set for a single hyper-parameter set\n",
    "crossval.results[0]['summary']['val']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQH-Aolud6r2"
   },
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45evlpBXd6r3",
    "outputId": "9b0d2389-3b70-4e4f-c4f6-fc9c9d7c5920"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED: EXECUTE CELL\n",
    "Obtain all the results for all parameters, for all sizes, for all\n",
    "rotations. This is the results attribute of the crossval object \n",
    "\"\"\"\n",
    "all_results = crossval.results\n",
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dBm9Faxd6r4",
    "outputId": "66465669-d6ec-415c-e7a2-852c87de68e8"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED: EXECUTE CELL\n",
    "Display the keys of the results object\n",
    "\"\"\"\n",
    "all_results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIvU6VpEd6r4",
    "outputId": "b403e80f-9f48-4eeb-a024-29b8cc8a2997"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED: EXECUTE CELL\n",
    "Obtain and display the indices of the best hyper-parameters for each training set size  \n",
    "using the 'best_param_inds' item from the crossval_report dict\n",
    "\"\"\"\n",
    "best_param_inds = crossval_report['best_param_inds']\n",
    "best_param_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-eT_9LeHd6r5",
    "outputId": "b72e2cd1-e209-4a3f-e411-ddc25ad4c6ba"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Display the list of the best parameter sets for each size. Use\n",
    "get_best_params_strings()\n",
    "\"\"\"\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "IPYY0rgBd6r-",
    "outputId": "54805ced-5e9e-43e9-a6f8-b51ee3d132c9"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Plot the validation results for all parameters over all train \n",
    "sizes, for the specified metrics. Use plot_allparams_val()\n",
    "\"\"\"\n",
    "metrics_used = ['fvaf_mean']\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "IPYY0rgBd6r-",
    "outputId": "54805ced-5e9e-43e9-a6f8-b51ee3d132c9"
   },
   "source": [
    "# Reflection, part 1\n",
    "\n",
    "## Q1: Explain the shape of the first curve listed in the legend (the blue curve).\n",
    "\n",
    "### TODO\n",
    "\n",
    "## Q2: Explain the shape of the curves as you scan from the top of the legend to the bottom.\n",
    "\n",
    "### TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "flspoXWJd6r7",
    "outputId": "5d293863-c055-4a27-f287-4e9738ed6df4"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED: EXECUTE CELL\n",
    "Plot the mean (summary) train and validation set performances for \n",
    "the best parameter set for each train size for the optimized\n",
    "metrics. Use plot_best_params_by_size()\n",
    "\"\"\"\n",
    "crossval.plot_best_params_by_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFZEKmqsd6r8"
   },
   "source": [
    "# Reflection, part 2\n",
    "\n",
    "## Q1: What changes in the selected hyper-parameters as we increase the number of training folds from 1 to 16?  Why?\n",
    "\n",
    "### TODO\n",
    "\n",
    "\n",
    "\n",
    "## Q2: Explain why FVAF increases monotonically for validation set performance, but not for training set performance.\n",
    "\n",
    "### TODO\n",
    "\n",
    "## Q3: What are the differences between the FVAF curves for the validation and test data sets?  Speculate as to why these curves are similar for the validation and test data sets.\n",
    "\n",
    "### TODO\n",
    "\n",
    "\n",
    "## Q4: What is the relationship between the validation set performance curve in this figure and the curves that are shown in the previous figure?\n",
    "\n",
    "### TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSRlTMm-d6sA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "homework6_sol2.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
